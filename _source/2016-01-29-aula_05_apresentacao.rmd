---
title: "Aula 05 - Modelando"
author: "Curso de R"
layout: post
tags: slides
category: slides
output: 
  ioslides_presentation:
    keep_md: false
    self_contained: true
---

```{r, echo=FALSE, warning=FALSE, message=FALSE}
library(pander)
library(lme4)
library(FactoMineR)
library(ggplot2)
library(magrittr)
library(tidyr)
library(dplyr)
panderOptions("table.style", "rmarkdown")
```

## Regressão Linear e ANOVA

- A função que ajusta modelo linear normal no R é `lm()`. 

- Você especifica o:
    - banco de dados; e 
    - a fórmula com as variáveis que deseja associar.
    
```{r, eval=FALSE}
ajuste <- lm(resposta ~ explicativas, data = meus_dados)
```

O objeto `ajuste` contém todos os resultados e com a ajuda de alguns comandos, você extrai tudo o que é interessante.

## Regressão Linear e ANOVA {.smaller}

Exemplo:
```{r}
ajuste_lm <- lm(mpg ~ wt, data = mtcars)
ajuste_lm
```

- `wt` é uma variável explicativa **contínua** (`numeric`), caracterizando um modelo de regressão. 
- Se `wt` fosse categórica (`factor`), estaríamos ajustando um modelo de ANOVA.

## Regressão Linear e ANOVA {.smaller}

Gráfico da reta ajustada:
```{r, fig.height=3}
# extrai os coeficientes ajustados
coeficientes <- coef(ajuste_lm)

ggplot(mtcars) +
  geom_point(aes(x = wt, y = mpg)) +
  geom_abline(intercept = coeficientes[1], slope = coeficientes[2])
```

## Regressão Linear e ANOVA {.smaller}

**summary()**

```{r, eval=FALSE}
summary(ajuste_lm)
```

![summary lm](figure/summary_lm.png)

## Regressão Linear e ANOVA {.smaller}

**summary()**

A função `summary()` é uma função genérica que geralmente devolve um resumo de informações úteis de praticamente qualquer classe de objetos. Para objetos `lm()` ela devolve:

1. Chamada do modelo
2. Medidas resumo dos resíduos
3. Tabela de coeficientes, desvios padrão e testes T para hipótese nula de parâmetros iguais a zero.
4. Média dos quadrados do resíduo e os respectivos graus de liberdade; $R^2$ e $R^2$ ajustado da regressão; Estatística F para qualidade do ajuste (comparação com o modelo com apenas o intercepto).

## Regressão Linear e ANOVA {.smaller}

**plot()**

A função `plot()` constrói gráficos úteis para diagnóstico do modelo.

```{r, eval=FALSE}
# opção para mostrar 4 gráficos em uma mesma figura
par(mfrow = c(2,2))

# gráficos de diagnóstico do modelo ajuste_lm
plot(ajuste_lm)

# retorna ao normal
par(mfrow = c(1,1))
```


## Regressão Linear e ANOVA {.smaller}

**plot()**

A função `plot()` constrói gráficos úteis para diagnóstico do modelo.

```{r, echo=FALSE}
# opção para mostrar 4 gráficos em uma mesma figura
par(mfrow = c(2,2))

# gráficos de diagnóstico do modelo ajuste_lm
plot(ajuste_lm)

# retorna ao normal
par(mfrow = c(1,1))
```

## Regressão Linear e ANOVA {.smaller}

**anova()**

Uma parte importante da modelagem é a redução de modelos. 

- A função `anova()` compara dois (ou mais) modelos encaixados por meio da estatística F (por padrão)

```{r, eval=FALSE}
# modelo nulo, com apenas o intercepto
ajuste_lm_nulo <- lm(mpg ~ 1, data = mtcars)
# compara o modelo com wt com o modelo nulo
anova(ajuste_lm_nulo, ajuste_lm)
```

## Regressão Linear e ANOVA {.smaller}

**anova()**

Uma parte importante da modelagem é a redução de modelos. 

- A função `anova()` compara dois (ou mais) modelos encaixados por meio da estatística F (por padrão)

```{r, echo=FALSE}
# modelo nulo, com apenas o intercepto
ajuste_lm_nulo <- lm(mpg ~ 1, data = mtcars)
# compara o modelo com wt com o modelo nulo
anova(ajuste_lm_nulo, ajuste_lm)
```

## Regressão Linear e ANOVA {.smaller}

**anova()**

- Caso seja passada apenas um ajuste à função, ela devolve a tabela de ANOVA (termos testados sequencialmente).

```{r, eval=FALSE}
# modelo com wt e cyl
ajuste_lm2 <- lm(mpg ~ wt + cyl, data = mtcars)
anova(ajuste_lm2)
```

## Regressão Linear e ANOVA {.smaller}

**anova()**

- Caso seja passada apenas um ajuste à função, ela devolve a tabela de ANOVA (termos testados sequencialmente).

```{r, echo=FALSE}
# modelo com wt e cyl
ajuste_lm2 <- lm(mpg ~ wt + cyl, data = mtcars)
anova(ajuste_lm2)
```

O valor-p = 0.001064 indica que o modelo com `wt + cyl` trás melhorias significantes no poder explicativo do modelo quando comparado ao modelo com apenas `wt`. 

## Regressão Linear e ANOVA {.smaller}

**stepwise**

Stepwise no R se faz com as funções `step()` do pacote `stats` ou `stepAIC()` do pacote `MASS`. Escolher entre `forward`, `backward` ou `both` (ambos) basta passar um desses nomes ao parâmetro `direction` da função.

```{r, message=FALSE, warning=FALSE, eval=FALSE}
# modelo aditivo completo
ajuste_lm_completo <- lm(mpg ~ ., data = mtcars)

# modelo forward
step(ajuste_lm_completo, direction = "forward")

# modelo backward
step(ajuste_lm_completo, direction = "backward")

# modelo both
step(ajuste_lm_completo, direction = "both")

```

## Regressão Linear e ANOVA {.smaller}

**demais comandos**

Outros comandos úteis são:

```{r, eval=TRUE, echo=FALSE, results='asis'}
comandos_uteis_lm <- data.frame("Função" = c("confint()",
                                             "resid()",
                                             "fitted()",
                                             "AIC()",
                                             "model.matrix()",
                                             "linearHypotesis()"),
                                "Descrição" = c("Intervalo de confiança para os parâmetros",
                                                "Resíduos do modelo",
                                                "Valores ajustados",
                                                "Critério de informação de Akaike",
                                                "Matriz de planejamento (matriz X) do modelo",
                                                "Teste de combinações lineares de parâmetros"))
comandos_uteis_lm %>% pander
```

## Regressão Linear Generalizada

MLG no R é igualmente fácil. Mudamos de `lm()` para `glm()` e acrescentamos a família de distribuições que queremos assumir para a resposta.

Situação:

- Suposição de normalidade não se verifica; e/ou 
- Homocedasticidade não se sustenta

Alternativa: regressão logística/Poisson/Gama...

## Regressão Linear Generalizada

**Modelo**

$$
Y_i \sim F(\mu_i, \phi)
$$

$$
\mu_i = g^{-1}(\alpha + \beta_1 x_{i1} + \beta_p x_{ip})
$$

## Regressão Linear Generalizada

**Estrutura**

```{r, eval=FALSE}
ajuste_glm <- glm(resposta ~ explicativas, 
                  data = dados, 
                  family = distribuicao)
```

Para ver a lista de distribuições que podem ser passadas ao parâmetro `family`, rode `?family` no R.

## Regressão Linear Generalizada

**Função de ligação**

Outro componente importante em modelos lineares generalizados é a função de ligação. De modo mais geral, o código para ajsutar um MLG fica assim:

```{r, eval=FALSE}
ajuste_glm <- glm(resposta ~ explicativas, 
                  data = dados, 
                  family = distribuicao(link = funcao_de_ligacao))
```

Repare na parte `(link = funcao_de_ligacao)` depois do nome da distribuição escolhida.

## Regressão Linear Generalizada

**Função de ligação**

Uma prática comum é trocar a ligação `inversa` de uma regressão Gama para uma ligação `log`, por exemplo. 

No R, fica:

```{r, eval=FALSE}
ajuste_gama <- glm(Y ~ X + I(X^2) + Z, 
                   data = dados, 
                   family = Gamma(link = "log"))
```

**OBS** Todas as funções úteis para `lm()` continuam sendo úteis para `glm()`.


## Regressão Linear Generalizada {.smaller}

**Exemplo: Regressão logística**

A regressão logística se caracteriza por assumir distribuição binomial à variável resposta. 

```{r}
# Regressão logistica: Ligação logit
ajuste_glm <- glm(am ~ wt, data = mtcars, family = binomial)
table(mtcars$am, predict(ajuste_glm) > 0.5)
```

## Regressão Linear Generalizada 

**Exemplo: Regressão logística**

- Por padrão, a função de ligação da distribuição `binomial` é a `logit`. 
- Se quisermos usar a função `probit`, precisamos especificar isto no parâmetro `family`.

```{r}
# Ligaçao probit
ajuste_glm_probit <- glm(am ~ wt, 
                         data = mtcars, 
                         family = binomial(link = "probit"))
```

## Regressão Linear Generalizada

**Exemplo: Regressão logística**

Gráfico das curvas ajustadas:
```{r, echo=FALSE, fig.height=4}
# library(ggplot2)
ggplot(mtcars, aes(x=wt, y=am)) + 
  geom_point() + 
  stat_smooth(aes(colour = "Logit"), 
              method="glm", method.args = list(family=binomial), se=FALSE) +
  stat_smooth(aes(colour = "Probit"), method="glm", 
              method.args =list(family=binomial(link = "probit")), se=FALSE) +
  stat_smooth(aes(colour = "Complementar Log-Log"), method="glm", 
              method.args = list(family=binomial(link = "cloglog")), se=FALSE) +
  labs(colour = "Função de ligação")
```

## Regressão Linear Generalizada {.smaller}

**Exemplo: Regressão logística**

O teste Chi quadrado é mais indicado para regressão logística.

```{r}
anova(ajuste_glm, test="Chisq")
```

## Modelos mistos

- Medidas Repetidas
- Dados longitudinais
- Observações dependentes
## Modelos mistos

**Estrutura**

```{r, eval=FALSE}
library(lme4)
ajuste_lmer <- lmer(resposta ~ explicativas + (form_ind|individuo), 
                    data = dados)
```

`form_ind` é a fórmula de como você quer ajustar o modelo para cada indivíduo (geralmente `1`, que é apenas intercepto aleatório) e `individuo` é a variável que identifica o indivíduo. 

## Modelos mistos

**Exemplo: Teste t pareado**

- Teste t pareado: comparar médias
- Banco de dados `sleep`.

```{r echo=FALSE}
sleep %>% ggplot() +
  geom_line(aes(x=group, y=extra, group=ID)) +
  geom_text(aes(x=group, y=extra, label=ID))
```

## Modelos mistos

**Exemplo: Teste t pareado**

Teste t pareado
```{r}
aux <- sleep %>% mutate(group=paste0('droga_', group)) %>% spread(group, extra)
t.test(aux$droga_1, aux$droga_2, paired=TRUE)
```

## Modelos mistos

**Exemplo: Teste t pareado**

Modelo misto equivalente

```{r}
modelo <- lmer(extra ~ group + (1|ID), data=sleep)
summary(modelo)
```

## Árvore de Decisão

- arvores de decisão unem simplicidade com eficiência. 
- No R seu ajuste é tão simples quanto ajustar um modelo de regressão.
- Possui um conjunto de funções igualmente convenientes para extrair resultados, basta conhecê-los (e iremos).

## Árvore de Decisão {.smaller}

- Vamos apresentar como ajustar arvore de decisão usando o pacote `tree`. 
- Porém, como tudo no R, existe inúmeros pacotes e jeitos de ajustar uma arvore de decisão (ver este [link](http://statistical-research.com/a-brief-tour-of-the-trees-and-forests/?utm_source=rss&utm_medium=rss&utm_campaign=a-brief-tour-of-the-trees-and-forests)).

**Ajuste**

```{r, eval=FALSE}
library(tree)
ajuste_tree <- tree(factor(am) ~ wt, data = mtcars)
summary(ajuste_tree)
```

## Árvore de Decisão {.smaller}

- Vamos apresentar como ajustar arvore de decisão usando o pacote `tree`. 
- Porém, como tudo no R, existe inúmeros pacotes e jeitos de ajustar uma arvore de decisão (ver este [link](http://statistical-research.com/a-brief-tour-of-the-trees-and-forests/?utm_source=rss&utm_medium=rss&utm_campaign=a-brief-tour-of-the-trees-and-forests)).

**Ajuste**

```{r, echo=FALSE}
library(tree)
ajuste_tree <- tree(factor(am) ~ wt, data = mtcars)
summary(ajuste_tree)
```

## Árvore de Decisão

- Vamos apresentar como ajustar arvore de decisão usando o pacote `tree`. 
- Porém, como tudo no R, existe inúmeros pacotes e jeitos de ajustar uma arvore de decisão (ver este [link](http://statistical-research.com/a-brief-tour-of-the-trees-and-forests/?utm_source=rss&utm_medium=rss&utm_campaign=a-brief-tour-of-the-trees-and-forests)).

**Grafico**

```{r, eval=FALSE}
plot(ajuste_tree)
text(ajuste_tree, pretty = 0)
```

## Árvore de Decisão

- Vamos apresentar como ajustar arvore de decisão usando o pacote `tree`. 
- Porém, como tudo no R, existe inúmeros pacotes e jeitos de ajustar uma arvore de decisão (ver este [link](http://statistical-research.com/a-brief-tour-of-the-trees-and-forests/?utm_source=rss&utm_medium=rss&utm_campaign=a-brief-tour-of-the-trees-and-forests)).

**Grafico**

```{r, echo=FALSE, fig.height=3}
plot(ajuste_tree)
text(ajuste_tree, pretty = 0)
```

## Árvore de Decisão

**comparação entre os modelos logístico e arvore**

Tabelas de observado versus predito:
```{r, eval=FALSE}
# Logistico
table(mtcars$am, predict(ajuste_glm) > 0.5)

# arvore
table(mtcars$am, predict(ajuste_tree)[,"1"] > 0.5)
```

## Árvore de Decisão

**Cross-validation**

- arvores tendem a "super-ajustar" (*overfit*) o modelo. 
- Para evitar este problema, *cross-validation* é uma boa pratica. 
- *Cross-validation* consiste em reservar parte da base reservada para depois ser usada para calcular o erro preditivo do modelo.
- O erro preditivo vira o novo critério de parada para a "podagem" da arvore.

## Árvore de Decisão

**Cross-validation**

Para fazer isso é muito facil com a função `cv.tree()`. Basta passar seu modelo ajustado para ela:

```{r, eval=FALSE}
set.seed(123)
cv_tree <- cv.tree(ajuste_tree)
plot(cv_tree)
```

## Árvore de Decisão

**Cross-validation**

Para fazer isso é muito facil com a função `cv.tree()`. Basta passar seu modelo ajustado para ela:

```{r, fig.height=4, echo=FALSE}
cv_tree <- cv.tree(ajuste_tree)
plot(cv_tree)
```

O grafico mostra qual tamanho da arvore que apresentou o menor erro de predição. No nosso caso foi tamanho `2`.

## Árvore de Decisão

**Cross-validation**

Para recuperar esse ajuste de tamanho `2`, chamamos a função `prune.tree()` da seguinte forma:

```{r, eval=FALSE}
# seleciona a arvoore com 2 nós
melhor_tree <- prune.tree(ajuste_tree, best = 2)
# Grafico que representa a arvore `melhor_tree`
plot(melhor_tree)
text(melhor_tree, pretty = 0)
```

## Árvore de Decisão

**Cross-validation**

Para recuperar esse ajuste de tamanho `2`, chamamos a função `prune.tree()` da seguinte forma:

```{r, echo=FALSE, fig.height=4}
# seleciona a arvoore com 2 nós
melhor_tree <- prune.tree(ajuste_tree, best = 2)
# Grafico que representa a arvore `melhor_tree`
plot(melhor_tree)
text(melhor_tree, pretty = 0)
```

## Árvore de Decisão {.smaller}

**Cross-validation**

Para recuperar esse ajuste de tamanho `2`, chamamos a função `prune.tree()` da seguinte forma:

